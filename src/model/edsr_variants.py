# EDSR çš„ PyTorch å®ç°ï¼Œä¸»è¦ç»“æ„æ˜¯ï¼š
# Headï¼šå·ç§¯ï¼ŒæŠŠè¾“å…¥å›¾åƒå˜æˆç‰¹å¾
# Bodyï¼šæ®‹å·®å—å †å ï¼ˆä¸»è¦å­¦ä¹ èƒ½åŠ›åœ¨è¿™é‡Œï¼‰
# Tailï¼šä¸Šé‡‡æ · + å·ç§¯ï¼Œè¾“å‡ºé«˜åˆ†è¾¨ç‡å›¾åƒ
# load_state_dictï¼šèƒ½è‡ªåŠ¨å…¼å®¹ä¸åŒæ”¾å¤§å€æ•°/é€šé“æ•°çš„é¢„è®­ç»ƒæ¨¡å‹
from model import common

import torch.nn as nn
import torch


class ECA(nn.Module):
    def __init__(self, k_size=3):
        super(ECA, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=k_size,
                              padding=(k_size - 1) // 2,
                              bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)                      # [B, C, 1, 1]
        y = y.squeeze(-1).transpose(-1, -2)       # [B, 1, C]
        y = self.conv(y)
        y = self.sigmoid(y).transpose(-1, -2).unsqueeze(-1)  # [B, C, 1, 1]
        return x * y.expand_as(x)
    
class CCA(nn.Module):
    def __init__(self, channel, reduction=16):
        super(CCA, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.var_pool = lambda x: torch.var(x, dim=(2, 3), keepdim=True)
        self.fc = nn.Sequential(
            nn.Conv2d(channel * 2, channel // reduction, 1, bias=True),
            nn.ReLU(inplace=True),
            nn.Conv2d(channel // reduction, channel, 1, bias=True),
            nn.Sigmoid()
        )
    def forward(self, x):
        avg = self.avg_pool(x)
        var = self.var_pool(x)
        att = self.fc(torch.cat([avg, var], dim=1))
        return x * att

class DW_PW_Conv(nn.Module):
    """
    Depthwise followed by Pointwise convolution with SiLU activations.
    DWConv -> SiLU -> PWConv -> SiLU
    """
    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):
        super(DW_PW_Conv, self).__init__()
        self.dw_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, groups=in_channels, bias=bias)
        self.act1 = nn.SiLU(inplace=True)
        self.pw_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=bias)
        self.act2 = nn.SiLU(inplace=True)
    def forward(self, x):
        x = self.dw_conv(x)
        x = self.act1(x)
        x = self.pw_conv(x)
        x = self.act2(x)
        return x

class MSDRB(nn.Module):
    """
    Multi-Stage Distillation Residual Block (è½»é‡è’¸é¦ç‰ˆ)
    - æ¯å±‚å·ç§¯åè’¸é¦å‡ºä¸€éƒ¨åˆ†ç‰¹å¾
    - ä¸å†ä½¿ç”¨å¤šå°ºåº¦å·ç§¯
    """
    def __init__(self, channels, distill_rate=0.25, act=nn.SiLU(inplace=True), ca=None, res_scale=1.0):
        super(MSDRB, self).__init__()
        self.res_scale = res_scale
        self.ca = ca
        self.act = act

        self.distilled_channels = int(channels * distill_rate)
        self.remaining_channels = channels - self.distilled_channels

        # 3 å±‚é€æ­¥è’¸é¦ç»“æ„ï¼Œæ›¿æ¢ä¸º DW_PW_Conv
        self.conv1 = DW_PW_Conv(channels, self.distilled_channels + self.remaining_channels)
        self.conv2 = DW_PW_Conv(self.remaining_channels, self.distilled_channels + self.remaining_channels)
        self.conv3 = DW_PW_Conv(self.remaining_channels, self.distilled_channels + self.remaining_channels)


        # èšåˆèåˆ
        self.fuse = nn.Sequential(
            nn.Conv2d(self.distilled_channels * 3, channels, 1, bias=True),
            nn.SiLU(inplace=True)
        )

    def forward(self, x):
        distilled_feats = []
        out = x

        # Stage 1
        out = self.act(self.conv1(out))
        d1, r1 = torch.split(out, [self.distilled_channels, self.remaining_channels], dim=1)
        distilled_feats.append(d1)

        # Stage 2
        out = self.act(self.conv2(r1))
        d2, r2 = torch.split(out, [self.distilled_channels, self.remaining_channels], dim=1)
        distilled_feats.append(d2)

        # Stage 3
        out = self.act(self.conv3(r2))
        d3, _ = torch.split(out, [self.distilled_channels, self.remaining_channels], dim=1)
        distilled_feats.append(d3)

        # èšåˆè’¸é¦ç‰¹å¾
        out = torch.cat(distilled_feats, dim=1)
        out = self.fuse(out)

        if self.ca is not None:
            out = self.ca(out)

        return x + out * self.res_scale
class EDSR(nn.Module):
    def __init__(self, args, conv=common.default_conv):
        super(EDSR, self).__init__()

        n_resblocks = args.n_resblocks   # æ®‹å·®å—æ•°é‡
        n_feats = args.n_feats           # æ¯å±‚ç‰¹å¾æ•°ï¼ˆé€šé“æ•°ï¼‰
        kernel_size = 3
        scale = args.scale[0]            # æ”¾å¤§å€æ•° (x2/x3/x4)
        act = nn.SiLU(inplace=True)        # æ¿€æ´»å‡½æ•°

        self.sub_mean = common.MeanShift(args.rgb_range)
        self.add_mean = common.MeanShift(args.rgb_range, sign=1)
        # sub_mean / add_mean â†’ å›¾åƒå½’ä¸€åŒ–/åå½’ä¸€åŒ–ï¼ˆå‡å»å‡å€¼å†åŠ å›æ¥ï¼‰ï¼Œè®­ç»ƒæ›´ç¨³å®šã€‚

        # define head module
        # ç¬¬ä¸€å±‚å·ç§¯ï¼šæŠŠè¾“å…¥ RGB å›¾åƒï¼ˆ3é€šé“ï¼‰æ˜ å°„åˆ° n_feats ä¸ªç‰¹å¾é€šé“ã€‚
        m_head = [conv(args.n_colors, n_feats, kernel_size)]

        # define body module (multi-stage feature aggregation)
        # ç”¨ msirb_blocks ä¿å­˜æ¯ä¸ª MSIRB å—
        self.msirb_blocks = nn.ModuleList([
            MSDRB(n_feats, act=act, ca=CCA(n_feats, reduction=4), res_scale=args.res_scale)
            for _ in range(n_resblocks)
        ])
        # èåˆå·ç§¯ï¼Œå°†æ‰€æœ‰æ®‹å·®å—è¾“å‡ºç‰¹å¾èšåˆ
        self.fuse_conv = nn.Conv2d(n_feats * n_resblocks, n_feats, 1, bias=True)

        # define tail module
        # Upsamplerï¼šä¸Šé‡‡æ ·æ¨¡å—ï¼ˆx2/x3/x4ï¼‰ï¼Œç”¨åƒç´ é‡æ’ï¼ˆpixel shuffleï¼‰å®ç°ã€‚
        # æœ€åä¸€å±‚å·ç§¯ï¼šæŠŠç‰¹å¾å›¾è½¬å› RGB å›¾åƒã€‚
        m_tail = [
            common.Upsampler(conv, scale, n_feats, act=False),
            conv(n_feats, args.n_colors, kernel_size)
        ]

        self.head = nn.Sequential(*m_head)
        self.tail = nn.Sequential(*m_tail)
    # è¾“å…¥å›¾åƒ â†’ å½’ä¸€åŒ– â†’ å·ç§¯æ˜ å°„ â†’ æ®‹å·®å—å †å  â†’ ä¸Šé‡‡æ · â†’ è¾“å‡ºè¶…åˆ†è¾¨å›¾åƒã€‚
    def forward(self, x):
        x = self.sub_mean(x)
        x_head = self.head(x)

        # å¤šå±‚ç‰¹å¾èšåˆ
        res_feats = []
        x = x_head
        for block in self.msirb_blocks:
            x = block(x)
            res_feats.append(x)
        # æ‹¼æ¥æ‰€æœ‰æ®‹å·®å—è¾“å‡º
        res_cat = torch.cat(res_feats, dim=1)
        fused = self.fuse_conv(res_cat)
        # æ®‹å·®è¿æ¥
        res = fused + x_head

        x = self.tail(res)
        x = self.add_mean(x)

        return x
    # è¿™æ˜¯è‡ªå®šä¹‰çš„æƒé‡åŠ è½½å‡½æ•°ï¼š
    # å¦‚æœæƒé‡ç»´åº¦åŒ¹é… â†’ æ­£å¸¸åŠ è½½
    # å¦‚æœä¸åŒ¹é…ä¸”ä¸æ˜¯ tail å±‚ â†’ æŠ¥é”™
    # tail å±‚æ˜¯æœ€åè¾“å‡ºå±‚ï¼ˆä¸åŒå€æ•°æ—¶å¤§å°å¯èƒ½ä¸åŒï¼‰ï¼Œæ‰€ä»¥å¯ä»¥å¿½ç•¥ä¸åŒ¹é…
    # ğŸ‘‰ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæœ‰æ—¶å€™çœ‹åˆ° strict=Falseï¼Œå®ƒå…è®¸â€œéƒ¨åˆ†åŠ è½½â€ï¼Œæ¯”å¦‚è¿ç§»å­¦ä¹ ã€‚
    def load_state_dict(self, state_dict, strict=True):
        own_state = self.state_dict()
        for name, param in state_dict.items():
            if name in own_state:
                if isinstance(param, nn.Parameter):
                    param = param.data
                try:
                    own_state[name].copy_(param)
                except Exception:
                    if name.find('tail') == -1:
                        raise RuntimeError('While copying the parameter named {}, '
                                           'whose dimensions in the model are {} and '
                                           'whose dimensions in the checkpoint are {}.'
                                           .format(name, own_state[name].size(), param.size()))
            elif strict:
                if name.find('tail') == -1:
                    raise KeyError('unexpected key "{}" in state_dict'
                                   .format(name))
def check_modules(net):
    params = sum(p.numel() for p in net.parameters())
    print(f"Params: {params/1e3:.1f}K")

def make_model(args, parent=False):
    net = EDSR(args)
    check_modules(net)
    return net
